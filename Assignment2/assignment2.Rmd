---
title: "Assignment 2"
author: "Group 61, Ikrame Zirar, Mohammed Majeed, Sergio Alejandro Gutierrez Maury"
date: "`r Sys.Date()`"
output: pdf_document
highlight: tango
---

```{r echo=FALSE}
options(digits=3)
```

## Excersice 1

**A)**

The dataset "treeVolume" contains a response variable, namely "Volume", and several explanatory variables, including "type", "height", and "diameter". To investigate the impact of tree type on volume, we conducted ANOVA using "Volume" as the response variable and "type" as the sole explanatory variable. The p-value from the ANOVA table indicates that there is no significant effect of tree type on tree volume.

```{r}
# Load the dataset
tree_data <- read.csv("treeVolume.txt", header = TRUE, sep = "")
tree_data$type = as.factor(tree_data$type)
# Perform anova test
model_aov <- aov(volume ~ type, data = tree_data)
summary(model_aov)
```

We conducted a t-test to compare the means of these two sample groups.the p-value of the t-test indicates that the type of tree does not have a significant impact on its volume.

```{r}
# Perform t-test
t_test <- t.test(volume ~ type, data = tree_data)
t_test
```

The output of aggregate gives us the estimated volumes for the two tree types

```{r}
# Estimate the volumes for the two tree types
aggregate(tree_data$volume, by = list(tree_data$type), mean)
```

**b)**

Include diameter and height as explanatory variables into the analysis and investigate whether the influence of diameter and height on volume is similar for both tree types. The ANOVA table for the model with explanatory variables diameter and type shows that diameter has a highly significant effect on volume (p-value \< 2.2e-16), but there is no significant interaction between diameter and type (p-value = 0.47). This suggests that the influence of diameter on volume is similar for both beech and oak trees.

The ANOVA table for the model with explanatory variables height and type shows that height has a highly significant effect on volume (p-value \< 2.2e-16), but there is no significant interaction between height and type (p-value = 0.18). This suggests that the influence of height on volume is similar for both beech and oak trees.

```{r}
# Fit a linear model with diameter, height, and type as explanatory variables
model_lm <- lm(volume ~ type + diameter + height, data = tree_data)

# Fit the model with diameter and type
model <- lm(volume ~ height + diameter*type  , data = tree_data)
# Test the significance of the interaction term
anova(model)

# Fit the model with height and type
model2 <- lm(volume ~ diameter + height*type , data = tree_data)
# apply anova
anova(model2)
```

Visualize the relationship between diameter, height, and volume for each tree type

```{r}
library(ggplot2)
ggplot(data = tree_data, aes(x = diameter, y = volume, color = type)) +
  geom_point() +
  labs(x = "Diameter", y = "Volume") +
  facet_wrap(~type)
ggplot(data = tree_data, aes(x = height, y = volume, color = type)) +
  geom_point() +
  labs(x = "Height", y = "Volume") +
  facet_wrap(~type)
```

**c)**

The coefficients for diameter and height are positive, indicating that there is a positive relationship between the tree's volume and its diameter and height. For every one unit increase in diameter, the predicted volume of the tree increases by 4.69806 units, and for every one unit increase in height, the predicted volume increases by 0.41725 units. The coefficient for "type" of the tree is -1.30460, but it is not statistically significant at the 0.05 level, as the p-value is 0.14. Therefore, we cannot conclude that the "type" of the tree has a significant effect on the tree's volume. the residual standard error, which is an estimate of the standard deviation of the errors, or the differences between the predicted values and the actual values. The lower the residual standard error, the better the model fits the data. The R-squared value is 0.9509, which means that the model explains 95.09% of the variance in the dependent variable. The F-statistic is 355, which is the ratio of the explained variance to the unexplained variance. The p-value for the F-statistic is less than 2.2e-16, indicating that the model is statistically significant overall.

```{r}
# Fit a linear model with diameter, height, and type as explanatory variables
model_lm2 <- lm(volume ~ type + diameter + height, data = tree_data)
summary(model_lm2)
```

investigate how diameter, height and type influence volume.

```{r}
# Plot the distribution of each variable
par(mfrow = c(2, 2))
hist(tree_data$diameter, main = "Diameter")
hist(tree_data$height, main = "Height")
hist(tree_data$volume, main = "Volume")
plot(tree_data$diameter, tree_data$height, main = "Scatterplot of Diameter and Height", xlab = "Diameter", ylab = "Height")

# Plot the relationship between diameter and volume
plot(tree_data$diameter, tree_data$volume, main = "Scatterplot of Diameter and Volume", xlab = "Diameter", ylab = "Volume")

# Plot the relationship between height and volume
plot(tree_data$height, tree_data$volume, main = "Scatterplot of Height and Volume", xlab = "Height", ylab = "Volume")

# Boxplot of volume by tree type
boxplot(volume ~ type, data = tree_data, main = "Boxplot of Volume by Tree Type", xlab = "Tree Type", ylab = "Volume")
```

Predict the volume for a tree with the (overall) average diameter and height.

```{r}
# Calculate the overall average diameter and height
avg_diameter <- mean(tree_data$diameter)
avg_height <- mean(tree_data$height)

# Predict the volume for a tree with the overall average diameter and height
predict(model_lm, newdata = data.frame(diameter = avg_diameter, height = avg_height, type = "beech"), interval = "confidence")
```

**d)** It seems there may be a natural relationship between the volume of a tree and its height and diameter. One possible transformation to consider is taking the logarithm of both height and diameter to create new variables, which may better capture the relationship with volume.

Both models have high R-squared values, indicating that they explain a large proportion of the variation in the response variable. However, the first model has a slightly higher R-squared value of 0.977 compared to the second model's (with no transformation) R-squared value of 0.951. This suggests that the first model may be a slightly better fit for the data.

```{r}

# fit a linear model with the transformed variables
transformed_model <- lm(log(volume) ~ log(tree_data$height) + log(tree_data$diameter) + type, data=tree_data)
# print the summary of the model to check the results
summary(transformed_model);

# fit a linear model with the transformed variables
model <- lm(volume ~ tree_data$height + tree_data$diameter + type, data=tree_data)
# print the summary of the model to check the results
summary(model);

```

## Excersice 2

**A)**

It can be seen from the box plots, that there are some outliers that might be influential, and from the leverage vs residuals plots we can see that, this is indeed the case.

```{r}
data <- read.csv("expensescrime.txt", header = TRUE, sep = " ")
# scatterplots
par(mfrow=c(3,2), mar=c(0.1,0.1,2,2))

boxplot(data$bad, data = data, notch = TRUE, varwidth = TRUE, main = "Bad")
boxplot(data$crime, data = data, notch = TRUE, varwidth = TRUE, main = "Crime")
boxplot(data$lawyers, data = data, notch = TRUE, varwidth = TRUE, main = "Lawyers")
boxplot(data$employ, data = data, notch = TRUE, varwidth = TRUE, main = "Employ")
boxplot(data$pop, data = data, notch = TRUE, varwidth = TRUE, main = "pop")
boxplot(data$expend, data = data, notch = TRUE, varwidth = TRUE, main = "Expend")
```

```{r}
par(mfrow=c(2,2), mar=c(2,1,2,2))
library(MASS)
rlm_fit1 <- lm(data$expend~data$bad, data=data)
rlm_fit2 <- lm(data$expend~data$crime, data=data)
rlm_fit3 <- lm(data$expend~data$lawyers, data=data)
rlm_fit4 <- lm(data$expend~data$employ, data=data)
rlm_fit5 <- lm(data$expend~data$pop, data=data)
plot(rlm_fit1)
plot(rlm_fit2)
plot(rlm_fit3)
plot(rlm_fit4)
plot(rlm_fit5)

```

By looking at the correlation matrix, it can be seen that there are some multicollinearity problems, since the variable "bad" is highly correlated with other independent variables.

```{r}
library("reshape2")
library('ggplot2')
# calculate correlation matrix
cor_matrix <- cor(data[, c("bad", "crime", "lawyers", "employ", "pop")])

# plot correlation matrix
ggplot(data = reshape2::melt(cor_matrix)) +
  geom_tile(aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(low = "blue", high = "red", midpoint = 0,
                       name = "Correlation") +
  theme_minimal() +
  labs(title = "Correlation Matrix of Explanatory Variables")

round(cor_matrix,2)
pairs(cor_matrix)

```

**B)**

The setp-up method selects as best model: $\hat{e}=\beta_{0}+\beta_{1} \cdot bad + \beta_{2} \cdot lawyers + \beta_{3} \cdot employ + \beta_{4} \cdot pop$

where all coefficients are significant with at least $5\%$ level.

```{r}
library(MASS)

# fit full model
full_model <- lm(expend ~ bad + crime + lawyers + employ + pop, data=data)

# step-up method to find best model
full_model.step <- stepAIC(full_model, direction="both")

summary(full_model.step)
```

**C)** 

The interval is: $ (-192.8264, 805.6644) $. In order to improve the accuracy of the prediction interval, we could explore alternative models by including additional variables, and evaluate if such models result in a reduction of the width of the prediction interval.

```{r}
# create new data frame with hypothetical values
new_data <- data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)

# predict expend using selected model
pred <- predict(full_model.step, newdata=new_data, interval="prediction", level=0.95)


pred
```

**D)**

Comparing the lasso model with the step-up model, the lasso model set the variables "bad" and "crime" to zero, which means that those variables are not important. As a result, we end up with a much simpler model.

```{r}
set.seed(73) #sheldon prime !
library(glmnet)
x <- as.matrix(data[, c("bad", "crime", "lawyers", "employ", "pop")])
y <- data$expend
train=sample(1:nrow(x),0.67*nrow(x)) # train by using 2/3 of the data
x.train=x[train,]; y.train=y[train] # data to train
x.test=x[-train,]; y.test=y[-train] # data to test the prediction quality


lasso.mod=glmnet(x.train,y.train,alpha=1)
cv.lasso=cv.glmnet(x.train,y.train,alpha=1,type.measure='mse')
plot(lasso.mod,label=T,xvar="lambda") #have a look at the lasso path
plot(cv.lasso) # the best lambda by cross-validation
plot(cv.lasso$glmnet.fit,xvar="lambda",label=T)
lambda.min=cv.lasso$lambda.min; lambda.1se=cv.lasso$lambda.1se
coef(lasso.mod,s=cv.lasso$lambda.min) #betaâ€™s for the best lambda
y.pred=predict(lasso.mod,s=lambda.min,newx=x.test) #predict for test
mse.lasso=mean((y.test-y.pred)^2) #mse for the predicted test rows

```

## Excersice 3

**A)**

```{r}
# install.packages("rms",dependencies = TRUE)
#install.packages("Hmisc")
#
library(ggplot2);
library(Hmisc);
library(rms);
library(rmsb);

```

-   For female passengers, there are a total of 462 observations and no missing values. Out of the 462 female passengers, 143 did not survive and 319 survived. Out of the female passengers who did not survive, 9 were from the 1st class, 13 were from the 2nd class, and 132 were from the 3rd class. Out of the female passengers who survived, 134 were from the 1st class, 94 were from the 2nd class, and 80 were from the 3rd class.

-   For male passengers, there are a total of 851 observations and no missing values. Out of the 851 male passengers, 468 did not survive and 383 survived. Out of the male passengers who did not survive, 120 were from the 1st class, 148 were from the 2nd class, and 441 were from the 3rd class. Out of the male passengers who survived, 59 were from the 1st class, 25 were from the 2nd class, and 58 were from the 3rd class.

```{r}
titanic_df <- read.table("titanic.txt", header=TRUE)
titanic_df$PClass <- as.factor(titanic_df$PClass)
titanic_df$Sex <- as.factor(titanic_df$Sex)
s= xtabs(~Survived + PClass + Sex, titanic_df)
plot(xtabs(~Survived + PClass + Sex, titanic_df))
s
```

```{r}
options(prType='html')
v <- c('PClass','Survived','Age','Sex')
titanic <- titanic_df[, v]
describe(titanic)
```

```{r}
# # spar(ps=4,rt=3)spar
dd <- datadist(titanic_df)
# describe distributions of variables to rms
options(datadist='dd')
s <- summary(Survived ~ Age + Sex + PClass , data=titanic_df)
plot(s, main='', subtitles=FALSE)

```

we can exponentiate their coefficients to get the odds ratios for survival. For example, the odds ratio for PClass2nd is exp(-1.29196) = 0.274, which suggests that passengers in second-class were 0.274 times as likely to survive as passengers in first-class. Similarly, the odds ratio for Age is exp(-0.03918) = 0.962, which means that for each one year increase in age, the odds of survival decrease by a factor of 0.962. The odds ratio for Sexmale is exp(-2.63136) = 0.072, which suggests that males were 0.072 times as likely to survive as females.

```{r}
model  <- glm(Survived ~ PClass + Age + Sex, data = titanic_df, family = binomial())
exp(coef(model))

summary(model)

```

```{r}
f <- lrm(Survived ~ Sex + PClass + rcs(Age,6), data=titanic_df)
p <- Predict(f, Age, Sex, PClass, fun=plogis)
plot <- ggplot(p)
plot + ggtitle("The predicted survival probability ")

```

**B)** Investigate the interaction of predictor Age with PClass.

```{r}
model4 <-glm(Survived ~ Age*PClass, data = titanic_df, family = binomial)
anova(model4, test="Chisq")
```

Investigate the interaction of predictor Age with Sex.

```{r}
model5 <-glm(Survived ~ Age*Sex , data = titanic_df, family = binomial)
anova(model5, test="Chisq")
```

Based on the analysis of deviance tables, it appears that PClass, Sex, and the interaction between Age and Sex are significant predictors of survival in the given dataset. The p-value for PClass was extremely small, indicating a very strong association between PClass and survival. Similarly, the p-value for Sex was likely also very small, given that it was reported as significant in the analysis. The interaction between Age and Sex was also found to be a significant predictor, which suggests that the relationship between Age and survival may differ depending on the individual's sex.

```{r}
# Fit a logistic regression model
model3 <- glm(Survived ~ PClass + Sex + Age:Sex, data = titanic_df, family = "binomial")
summary(model3)
```

The table provides the survival probabilities for six different combinations of PClass, Sex, and Age, based on the model used to analyze the Titanic dataset. according to the table, a 55-year-old male passenger in 1st class had a survival probability of 0.1450, while a 55-year-old female passenger in 1st class had a much higher survival probability of 0.9474. Similarly, a 55-year-old male passenger in 3nd class had a very low survival probability of 0.0118, while a 55-year-old female passenger in 3nd class had a much higher survival probability of 0.5590.

```{r}
# Create a new dataset with all possible combinations of PClass, Sex, and Age
newdata <- expand.grid(PClass = c("1st", "2nd", "3rd"),
                       Sex = c("male", "female"),
                       Age = 55)
# Add a column with predicted survival probabilities
newdata$Survival_Probability <- predict(model3, newdata, type = "response")
head(newdata)
```

```{r}
p<- ggplot(newdata, aes(x = PClass, y = Survival_Probability, fill = Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_bw()
p + ggtitle("Probability of survival for age 55")
```

**C)**

We could use Logistic Regression to model the probability of a certain passenger surviving or not. To evaluate the model, we could use $R^2$ or Accuracy. To implement the model, we would need to clean the dataset, handling missing values, encoding the categorical variables, and normalizing the numerical variables.

**D)**

We want to test whether H0: row variable and column variable are independent. The p-values are \<0.05, so we reject the H0.

```{r}
contclass=xtabs(~PClass+Survived, data=titanic_df)
contclass

chisq.test(contclass)
```

```{r}
contsex=xtabs(~Sex+Survived, data=titanic_df)
contsex

chisq.test(contsex)
```

**E)**
Contingency tables are for checking independence or to check if distributions are homogeneous, so yes we would say it is not the best way for prediction.

An advantage and disadvantage of Logistic Regression and contingency tables relative to each other are that contingency tables are easier compared to LR, but they lack the ability to model more complex relationships, which LR is able to do.

## Excersice 4

**A)**

We check for correlation between all pairs of variables.The plot shows that there is no correlation.

We perform Poisson regression and find that oligarchy, pollib and parties have a significant effect on miltcoup, because their p-values are \<0.05.

```{r}
data=read.table(file= "coups.txt", header=TRUE)
pairs(data[,-1])
```

```{r, fig.width=6,fig.height=3.5,collapse=TRUE}
glmcoups=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family=poisson, data=data)
summary(glmcoups)
```

**B)**

We will use the step-down approach to reduce the number of explanatory variables. This means we keep the variables that have the most significant effect. Analyzing the summaries, we iterate through and remove the variables with the highest p-values. From A, we start with removing numelec because it has the highest p-value and is \>0.05. Next, we remove numregime, then size, popn and lastly pctvote. We stop here since all p-values are \< 0.05 and thus are significant. The final model is: miltcoup=0.25138 + 0.09262*oligarchy - 0.57410*pollib + 0.02206\*parties + error.

Comparing the results to a), the step down approach model shows similar results, the same variables show a sifnificant effect on miltcoup.

```{r, fig.width=6,fig.height=3.5,collapse=TRUE}
glmcoups2=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim, family=poisson, data=data)
#summary(glmcoups2) #numregime: 0.4264   

glmcoups2=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family=poisson, data=data)
#summary(glmcoups2) #remove size: 0.42138    

glmcoups2=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=data)
#summary(glmcoups2) #remove popn: 0.2988

glmcoups2=glm(miltcoup~oligarchy+pollib+parties+pctvote, family=poisson, data=data)
#summary(glmcoups2) #remove pctvote: 0.1803

glmcoups2=glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=data)
summary(glmcoups2)
```

**C)**

The findings show that predicted average of coups per country increases as the political liberalization decreases.

```{r}
avg1 =0.25138+0.09262*mean(data$oligarchy)-0.57410*0+0.02206*mean(data$parties)
avg2 =0.25138+0.09262*mean(data$oligarchy)-0.57410*1+0.02206*mean(data$parties)
avg3 =0.25138+0.09262*mean(data$oligarchy)-0.57410*2+0.02206*mean(data$parties)
avg =c(exp(avg1), exp(avg2), exp(avg3))
avg1; avg2; avg3; avg
```
